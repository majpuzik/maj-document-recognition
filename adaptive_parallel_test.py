#!/usr/bin/env python3
"""
NAS5 Docker Apps Collection
"""

"""
This project implements a Model Context Protocol (MCP) server that allows interaction with Gmail accounts via IMAP and SMTP. It provides tools for searching emails, retrieving content, managing labels
"""

"""
This project implements a Model Context Protocol (MCP) server that allows interaction with Gmail accounts via IMAP and SMTP. It provides tools for searching emails, retrieving content, managing labels
"""

"""
ADAPTIVE parallel processing - dynamically adjusts workers based on system load
Processes up to 2000 emails from Thunderbird
"""

import sys
import psutil
import time
import threading
from pathlib import Path
from collections import Counter
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
import logging
import email
import mailbox
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from src.ocr.document_processor import DocumentProcessor
from src.ai.classifier_improved import ImprovedAIClassifier
from src.database.db_manager import DatabaseManager
from src.integrations.blacklist_whitelist import BlacklistWhitelist

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/adaptive_test.log"),
        logging.StreamHandler(sys.stdout),
    ]
)
logger = logging.getLogger(__name__)

class ResourceMonitor:
    """Monitor system resources and manage worker count"""

    def __init__(self, max_cpu=90, max_mem=90, check_interval=10):
        self.max_cpu = max_cpu
        self.max_mem = max_mem
        self.check_interval = check_interval
        self.should_reduce = False
        self.current_workers = 0
        self.monitoring = False

    def check_resources(self):
        """Check if resources are within limits"""
        cpu = psutil.cpu_percent(interval=1)
        mem = psutil.virtual_memory().percent

        cpu_ok = cpu < self.max_cpu
        mem_ok = mem < self.max_mem

        return {
            "cpu": cpu,
            "mem": mem,
            "cpu_ok": cpu_ok,
            "mem_ok": mem_ok,
            "safe": cpu_ok and mem_ok
        }

    def start_monitoring(self, executor, futures_list):
        """Start background monitoring"""
        self.monitoring = True

        def monitor_loop():
            while self.monitoring and futures_list:
                time.sleep(self.check_interval)

                resources = self.check_resources()
                active = sum(1 for f in futures_list if not f.done())

                logger.info(f"üìä Monitor: CPU={resources['cpu']:.1f}% MEM={resources['mem']:.1f}% Active={active}")

                if not resources['safe']:
                    logger.warning(f"‚ö†Ô∏è OVERLOAD! CPU={resources['cpu']:.1f}% MEM={resources['mem']:.1f}%")

                    # Cancel some futures to reduce load
                    cancelled = 0
                    for f in reversed(futures_list):
                        if not f.done() and not f.running():
                            f.cancel()
                            cancelled += 1
                            if cancelled >= 2:  # Cancel 2 at a time
                                break

                    if cancelled > 0:
                        logger.warning(f"üõë Cancelled {cancelled} pending tasks to reduce load")

                    time.sleep(5)  # Wait before next check

        thread = threading.Thread(target=monitor_loop, daemon=True)
        thread.start()
        return thread

def load_config():
    import yaml
    with open("config/config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    # BEST MODEL
    config['ai']['ollama']['model'] = 'qwen2.5:32b'
    config['ai']['ollama']['temperature'] = 0.05
    config['ai']['ollama']['timeout'] = 180

    return config

def extract_from_multiple_mailboxes(profile_path, temp_dir, limit=2000, max_size_mb=3):
    """Extract from multiple mailboxes"""

    mailboxes = [
        profile_path / "ImapMail/outlook.office365.com/INBOX",
        profile_path / "ImapMail/outlook.office365.com/Archive",
        profile_path / "ImapMail/outlook.office365.com/Archivovat",
        profile_path / "ImapMail/outlook.office365.com/Sent-1",
    ]

    all_attachments = []
    max_size_bytes = max_size_mb * 1024 * 1024

    for mailbox_path in mailboxes:
        if not mailbox_path.exists():
            logger.warning(f"Mailbox not found: {mailbox_path.name}")
            continue

        if len(all_attachments) >= limit:
            break

        logger.info(f"\nüì¨ Scanning: {mailbox_path.name}")

        try:
            mbox = mailbox.mbox(str(mailbox_path))
            count = 0

            for idx, msg in enumerate(mbox):
                if len(all_attachments) >= limit:
                    break

                sender = msg.get("From", "")
                subject = msg.get("Subject", "")

                for part in msg.walk():
                    if len(all_attachments) >= limit:
                        break

                    if part.get_content_maintype() == "multipart":
                        continue

                    filename = part.get_filename()
                    if not filename:
                        continue

                    ext = Path(filename).suffix.lower()
                    if ext not in ['.pdf', '.jpg', '.jpeg', '.png']:
                        continue

                    try:
                        payload = part.get_payload(decode=True)
                        if len(payload) > max_size_bytes:
                            continue

                        timestamp = int(datetime.now().timestamp() * 1000000)
                        safe_filename = f"doc_{len(all_attachments)}_{timestamp}_{filename}"
                        attachment_path = temp_dir / safe_filename

                        with open(attachment_path, "wb") as f:
                            f.write(payload)

                        all_attachments.append({
                            "path": str(attachment_path),
                            "filename": filename,
                            "sender": sender,
                            "subject": subject,
                            "mailbox": mailbox_path.name,
                            "size_kb": len(payload) / 1024
                        })

                        count += 1

                        if count % 50 == 0:
                            logger.info(f"  [{len(all_attachments)}/{limit}] extracted from {mailbox_path.name}")

                    except Exception as e:
                        logger.debug(f"Error extracting: {e}")

            logger.info(f"‚úì {mailbox_path.name}: {count} attachments")

        except Exception as e:
            logger.error(f"Mailbox error {mailbox_path.name}: {e}")

    return all_attachments

def process_single_document(args):
    """Process one document"""
    attachment, config, idx, total = args

    processor = DocumentProcessor(config)
    db = DatabaseManager(config)
    classifier = ImprovedAIClassifier(config, db)
    blacklist_whitelist = BlacklistWhitelist(config)

    result = {
        "idx": idx,
        "filename": attachment['filename'],
        "success": False,
    }

    try:
        # OCR
        ocr_result = processor.process_document(attachment["path"])

        if not ocr_result.get("success"):
            result["error"] = "OCR failed"
            return result

        text = ocr_result.get("text", "")
        ocr_conf = ocr_result.get("confidence", 0)

        # AI
        classification = classifier.classify(text, ocr_result.get("metadata", {}))

        doc_type = classification.get("type", "jine")
        ai_conf = classification.get("confidence", 0)

        # Save
        doc_id = db.insert_document(
            file_path=attachment["path"],
            ocr_text=text,
            ocr_confidence=ocr_conf,
            document_type=doc_type,
            ai_confidence=ai_conf,
            metadata={
                **classification.get("metadata", {}),
                "sender": attachment["sender"],
                "subject": attachment["subject"],
                "mailbox": attachment["mailbox"],
            }
        )

        result["success"] = True
        result["doc_type"] = doc_type
        result["confidence"] = ai_conf
        result["db_id"] = doc_id

        logger.info(f"[{idx}/{total}] ‚úì {doc_type} ({ai_conf:.0%}) - {attachment['filename'][:40]}")

        return result

    except Exception as e:
        logger.error(f"[{idx}/{total}] ‚úó {e}")
        result["error"] = str(e)
        return result

def main():
    start_time = time.time()

    logger.info("="*80)
    logger.info("üöÄ ADAPTIVE PARALLEL PROCESSING - 2000 EMAILS")
    logger.info("Model: qwen2.5:32b (72.7B parameters)")
    logger.info("Max workers: Adaptive (1-6 based on system load)")
    logger.info("="*80)

    # Setup
    config = load_config()
    temp_dir = Path("data/temp_adaptive")
    temp_dir.mkdir(parents=True, exist_ok=True)

    # Extract
    logger.info("\nüìß EXTRACTING EMAILS...")
    profile_path = Path("/Users/m.a.j.puzik/Library/Thunderbird/Profiles/1oli4gwg.default-esr")

    attachments = extract_from_multiple_mailboxes(
        profile_path,
        temp_dir,
        limit=2000,
        max_size_mb=3
    )

    logger.info(f"\n‚úì Extracted {len(attachments)} documents from {len(set(a['mailbox'] for a in attachments))} mailboxes")

    if not attachments:
        logger.error("No attachments!")
        return

    # Initial resource check - longer interval to reduce spam
    monitor = ResourceMonitor(max_cpu=90, max_mem=90, check_interval=30)
    resources = monitor.check_resources()

    logger.info(f"\nInitial state: CPU={resources['cpu']:.1f}% MEM={resources['mem']:.1f}%")

    # Determine initial workers - AGGRESSIVE START
    # CPU has headroom (50-70%), so start with 4 workers
    initial_workers = 4

    logger.info(f"Starting with {initial_workers} workers (aggressive mode)")

    # Process
    logger.info(f"\nüîÑ PROCESSING {len(attachments)} DOCUMENTS...\n")

    process_args = [(att, config, i+1, len(attachments)) for i, att in enumerate(attachments)]

    results = []
    completed = 0

    with ProcessPoolExecutor(max_workers=initial_workers) as executor:
        # Submit all
        futures_list = [executor.submit(process_single_document, args) for args in process_args]

        # Start monitoring
        monitor.start_monitoring(executor, futures_list)

        # Collect results
        for future in as_completed(futures_list):
            try:
                result = future.result(timeout=300)  # 5 min timeout per doc
                results.append(result)
                completed += 1

                if completed % 20 == 0:
                    res = monitor.check_resources()
                    logger.info(f"\nüìä Progress: {completed}/{len(attachments)} | CPU={res['cpu']:.1f}% MEM={res['mem']:.1f}%\n")

            except Exception as e:
                logger.error(f"Task failed: {e}")
                completed += 1

    monitor.monitoring = False

    # Stats
    total_time = time.time() - start_time
    successful = sum(1 for r in results if r.get("success"))

    type_counts = Counter()
    total_conf = 0

    for r in results:
        if r.get("success"):
            type_counts[r["doc_type"]] += 1
            total_conf += r["confidence"]

    avg_conf = total_conf / successful if successful > 0 else 0

    # Report
    logger.info(f"\n{'='*80}")
    logger.info("üéâ FINAL RESULTS")
    logger.info(f"{'='*80}")
    logger.info(f"‚è±Ô∏è  Total time: {total_time:.0f}s ({total_time/60:.1f} min)")
    logger.info(f"‚ö° Avg/doc: {total_time/len(attachments):.1f}s")
    logger.info(f"üìÑ Processed: {len(results)}/{len(attachments)}")
    logger.info(f"‚úì Success: {successful}")
    logger.info(f"‚úó Failed: {len(results) - successful}")
    logger.info(f"üéØ Avg confidence: {avg_conf:.1%}")

    logger.info(f"\nüìä Top 10 document types:")
    for doc_type, count in type_counts.most_common(10):
        pct = (count / successful * 100) if successful > 0 else 0
        logger.info(f"  {doc_type:<25} {count:>4} ({pct:>5.1f}%)")

    logger.info(f"\nüíæ Database: data/documents.db")
    logger.info(f"üìÅ Total documents in DB: {successful + 24}")  # +24 from previous tests

    logger.info(f"{'='*80}\n")

    # Cleanup
    logger.info("üßπ Cleaning up...")
    cleaned = 0
    for att in attachments:
        try:
            Path(att["path"]).unlink(missing_ok=True)
            cleaned += 1
        except:
            pass
    logger.info(f"‚úì Cleaned {cleaned} temp files")

if __name__ == "__main__":
    mp.set_start_method('spawn', force=True)
    main()
